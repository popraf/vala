{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment Analysis\n",
    "This notebook leverages a NLP model pre-trained on engligh reviews. Reviews are firstly translated to English, using `Narrativa/mbart-large-50-finetuned-opus-pt-en-translation`, along with language identification via `facebook/fasttext-language-identification1` and then classified within `cardiffnlp/twitter-xlm-roberta-base-sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries setup\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup pytorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Download the WordNet resource\n",
    "# nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "fpath_data = os.environ.get(\"FPATH_DATA\")\n",
    "\n",
    "# Load reviews data\n",
    "reviews = pd.read_csv(fpath_data+\"order_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like all reviews are from Brazil according to customers.csv\n",
    "\n",
    "# Convert to datetime column review_creation_date\n",
    "reviews['review_creation_date'] = pd.to_datetime(reviews['review_creation_date'])\n",
    "\n",
    "# For the sake of simplicit, lets merge review comment title and message\n",
    "reviews['review_concat'] = reviews['review_comment_title'].fillna('')+', '+reviews['review_comment_message'].fillna('')\n",
    "\n",
    "# # Drop rows with null ['review_concat']\n",
    "# reviews = reviews.dropna(subset=['review_concat'])\n",
    "\n",
    "# Drop rows with only ', ' string\n",
    "reviews = reviews[reviews.review_concat != ', ']\n",
    "\n",
    "# Lets keep only relevant columns\n",
    "reviews = reviews[['review_id', 'order_id', 'review_concat','review_creation_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check if there are any numbers in the reviews - this will be helpfull for the second task\n",
    "def search_numbers(text):\n",
    "    return bool(re.search(r'\\d+', text))\n",
    "\n",
    "reviews['contain_num'] = reviews['review_concat'].apply(search_numbers)\n",
    "# reviews[reviews['contain_num']==True].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis from the text.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   u\"\\U0001f926-\\U0001f937\"\n",
    "                                   u\"\\U00010000-\\U0010ffff\"\n",
    "                                   u\"\\u2640-\\u2642\"\n",
    "                                   u\"\\u2600-\\u2B55\"\n",
    "                                   u\"\\u200d\"\n",
    "                                   u\"\\u23cf\"\n",
    "                                   u\"\\u23e9\"\n",
    "                                   u\"\\u231a\"\n",
    "                                   u\"\\ufe0f\"  # dingbats\n",
    "                                   u\"\\u3030\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_ascii(text):\n",
    "    def is_supported(char):\n",
    "        try:\n",
    "            char.encode(encoding='utf-8').decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def clean_string(s):\n",
    "        return ''.join(c for c in s if is_supported(c) or unicodedata.category(c) not in ['Cn', 'Co', 'Cs'])\n",
    "\n",
    "    cleaned_list = [clean_string(s) for s in text]\n",
    "    return [s for s in cleaned_list if s]  # Remove any empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens, lang):\n",
    "    \"\"\"Eliminate common stopwords from the tokenized text.\n",
    "    Stop words are commonly used words like \"the,\" \"is,\" or \n",
    "    \"and\" that don't carry much meaning and can be removed to reduce noise in the data\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(lang)) # Stopwords language\n",
    "    return [word for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"Split the text into individual words or tokens.\"\"\"\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    \"\"\"Exclude numerical digits from the text.\"\"\"\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation marks from the text.\"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_extra_whitespaces(text):\n",
    "    \"\"\"Remove extra white space from text.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "def remove_single_chars(text):\n",
    "    \"\"\"Remove all single characters from text\"\"\"\n",
    "    return re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    \"\"\"Remove all the special characters from text.\"\"\"\n",
    "    return re.sub(r'\\W', ' ', text)\n",
    "\n",
    "def remove_not_alphabetical(text):\n",
    "    \"\"\"Remove any character that isn't alphabetical.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final preprocessing pipeline, some of the functions might not be used in the solution, althrough defined\n",
    "def preprocess_en(text):\n",
    "    text = str(text).lower() # To lower\n",
    "    text = remove_extra_whitespaces(text)\n",
    "    # text = remove_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = tokenize_text(text)\n",
    "    text = remove_stopwords(text, 'english')\n",
    "    text = remove_emojis(text)\n",
    "    text = keep_only_ascii(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Translation to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Narrativa/mbart-large-50-finetuned-opus-pt-en-translation\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "tokenizer.src_lang = 'pt_XX' # Further should be expanded with facebook/fasttext-language-identification to identify language instead of hardcoding PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_pt_to_en(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids.to('cuda')\n",
    "    attention_mask = inputs.attention_mask.to('cuda')\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, forced_bos_token_id=tokenizer.lang_code_to_id['en_XX'])\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"Translating\")\n",
    "reviews['reviews_en'] = reviews['review_concat'].progress_apply(translate_pt_to_en)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prerocessing of english reviews\n",
    "Step applied after the translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['reviews_en_preprocessed'] = reviews['reviews_en'].apply(preprocess_en)\n",
    "reviews['reviews_en_preprocessed_str'] = reviews['reviews_en_preprocessed'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv('reviews_en.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_aio = ' '.join(reviews['reviews_en_preprocessed_str'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(reviews_aio)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Translated Reviews Word Cloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "reviews['review_length'] = reviews['reviews_en'].str.len()\n",
    "\n",
    "# Histogram of review length\n",
    "ax.hist(reviews['review_length'], bins=20, edgecolor='black')\n",
    "ax.set_title('Histogram of a Review String Length')\n",
    "ax.set_xlabel('Review String Length')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment analysis\n",
    "As mentioned in the beginning, this notebook leverages `ramonmedeiro1/bertimbau-products-reviews-pt-br` pre-trained model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ramonmedeiro1/bertimbau-products-reviews-pt-br\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classes = ['Very Negative', 'Negative', 'Neutral', 'Positive', 'Very Positive']\n",
    "\n",
    "def get_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    predicted_sentiment = sentiment_classes[predicted_class]\n",
    "\n",
    "    return predicted_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['sentiment'] = reviews['reviews_processed_str'].apply(get_sentiment)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = reviews.set_index('review_creation_date')\n",
    "sentiment_over_time = results.resample('ME')['sentiment'].value_counts().unstack().fillna(0)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for sentiment in sentiment_over_time.columns:\n",
    "    plt.plot(sentiment_over_time.index, sentiment_over_time[sentiment], label=sentiment, marker='o')\n",
    "\n",
    "plt.title('Sentiment of Reviews by Year Month')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud for a very positive sentiment\n",
    "reviews_aio = ' '.join(reviews[reviews['sentiment']=='Very Positive']['reviews_processed_str'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(reviews_aio)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Very Positive Reviews Word Cloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud for a very negative sentiment\n",
    "reviews_aio = ' '.join(reviews[reviews['sentiment']=='Very Negative']['reviews_processed_str'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(reviews_aio)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Very Negative Reviews Word Cloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vala",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
